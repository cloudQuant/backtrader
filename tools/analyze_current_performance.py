#!/usr/bin/env python
"""Performance Log Analysis Tool for Backtrader Optimization.

This module provides tools for analyzing Python profiling logs generated by
cProfile or similar profilers. It parses performance logs, identifies bottlenecks,
compares performance with baseline measurements, and generates optimization
recommendations.

Key Features:
    - Parse Python profiler output logs
    - Identify performance bottlenecks in critical functions
    - Compare current performance with baseline measurements
    - Generate prioritized optimization recommendations
    - Export analysis reports to markdown format

The tool focuses on identifying common performance issues in Python code:
    - Excessive hasattr/getattr/setattr calls (LBYL vs EAFP patterns)
    - Expensive __getattr__/__setattr__ magic method usage
    - Type checking overhead (isinstance, isnan)
    - Array indexing overhead (__getitem__)

Example:
    >>> python tools/analyze_current_performance.py
    This will analyze the latest performance log and generate recommendations.
"""

import re
import sys
from collections import defaultdict


def parse_log_file(filename):
    """Parse a Python profiler log file and extract performance metrics.

    This function reads a cProfile-generated log file and extracts:
    - Total execution time and function call count
    - Per-function statistics including call count, time spent, and source location

    Args:
        filename (str): Path to the profiler log file to parse.

    Returns:
        dict: A dictionary containing:
            - total_calls (int): Total number of function calls
            - total_time (float): Total execution time in seconds
            - functions (list): List of function statistics, each containing:
                - ncalls (int): Number of calls
                - tottime (float): Total time in function (excluding subcalls)
                - cumtime (float): Cumulative time (including subcalls)
                - filename (str): Source file name
                - lineno (str): Line number
                - funcname (str): Function name
                - fullname (str): Full identifier (filename:lineno(funcname))

    Raises:
        FileNotFoundError: If the log file cannot be read.
        Exception: If the log file format is invalid.
    """
    with open(filename, encoding="utf-8") as f:
        content = f.read()

    # Extract overall information
    # Matches format: "1234567 function calls (1234567 primitive calls) in 12.34 seconds"
    total_calls_match = re.search(r"(\d+)\s+function calls.*in\s+([\d.]+)\s+seconds", content)
    if total_calls_match:
        total_calls = int(total_calls_match.group(1))
        total_time = float(total_calls_match.group(2))
    else:
        total_calls, total_time = 0, 0.0

    # Extract function statistics
    functions = []

    # Pattern matches profiler output table rows:
    # ncalls  tottime  percall_tot  cumtime  percall_cum  filename:lineno(funcname)
    # Example: "1865/1660  0.003  1.6e-06  0.014  7.5e-06  file.py:123(function_name)"
    pattern = (
        r"\s+(\d+(?:/\d+)?)\s+([\d.]+)\s+([\d.]+)\s+([\d.]+)\s+([\d.]+)\s+([^:]+):(\d+)\(([^)]+)\)"
    )

    for match in re.finditer(pattern, content):
        ncalls = match.group(1)
        tottime = float(match.group(2))
        percall_tot = float(match.group(3))
        cumtime = float(match.group(4))
        percall_cum = float(match.group(5))
        filename = match.group(6)
        lineno = match.group(7)
        funcname = match.group(8)

        # Parse ncalls (may contain recursive calls like "1865/1660")
        # Format: "total_calls/primitive_calls"
        if "/" in ncalls:
            calls, primitive = ncalls.split("/")
            ncalls_num = int(calls)
        else:
            ncalls_num = int(ncalls)

        functions.append(
            {
                "ncalls": ncalls_num,
                "tottime": tottime,
                "cumtime": cumtime,
                "filename": filename,
                "lineno": lineno,
                "funcname": funcname,
                "fullname": f"{filename}:{lineno}({funcname})",
            }
        )

    return {"total_calls": total_calls, "total_time": total_time, "functions": functions}


def analyze_bottlenecks(log_data):
    """Analyze performance bottlenecks from profiler data.

    Identifies and categorizes performance bottlenecks by:
    1. Ranking functions by cumulative time consumption
    2. Grouping critical bottleneck functions by type (hasattr, getattr, etc.)
    3. Calculating time distribution and call counts for each category

    Args:
        log_data (dict): Parsed log data from parse_log_file().

    Returns:
        dict: A dictionary of bottleneck categories, each containing a list of
            functions in that category. Keys include:
            - hasattr: Functions related to attribute checking
            - getattr: Functions related to attribute getting
            - setattr: Functions related to attribute setting
            - isinstance: Type checking functions
            - isnan: NaN checking functions
            - __getattr__: Custom __getattr__ implementations
            - __setattr__: Custom __setattr__ implementations
            - __getitem__: Array indexing operations
            - forward: Forward method calls

    Example:
        >>> log_data = parse_log_file("profile.log")
        >>> bottlenecks = analyze_bottlenecks(log_data)
        >>> print(f"hasattr calls: {sum(f['ncalls'] for f in bottlenecks['hasattr'])}")
    """
    functions = log_data["functions"]

    print("\n" + "=" * 100)
    print("Current Performance Bottleneck Analysis")
    print("=" * 100)
    print(f"\nTotal execution time: {log_data['total_time']:.2f}s")
    print(f"Total function calls: {log_data['total_calls']:,}")
    print(f"Average per call: {(log_data['total_time']/log_data['total_calls']*1000000):.2f} microseconds")

    # Sort by cumulative time and display top 20
    print("\n" + "-" * 100)
    print("TOP 20 Most Time-Consuming Functions (by cumulative time)")
    print("-" * 100)
    print(f"{'Rank':<5} {'Function':<60} {'Calls':<15} {'Cum Time':<12} {'Pct':<8}")
    print("-" * 100)

    sorted_by_cumtime = sorted(functions, key=lambda x: x["cumtime"], reverse=True)[:20]
    for i, func in enumerate(sorted_by_cumtime, 1):
        percent = func["cumtime"] / log_data["total_time"] * 100
        print(
            f"{i:<5} {func['funcname']:<60} {func['ncalls']:>14,} {func['cumtime']:>11.3f}s {percent:>7.1f}%"
        )

    # Critical bottleneck function analysis
    # Groups functions by common performance anti-patterns
    print("\n" + "-" * 100)
    print("Critical Bottleneck Functions Detailed Analysis")
    print("-" * 100)

    bottlenecks = {
        "hasattr": [],
        "getattr": [],
        "setattr": [],
        "isinstance": [],
        "isnan": [],
        "__getattr__": [],
        "__setattr__": [],
        "__getitem__": [],
        "forward": [],
    }

    # Categorize functions by bottleneck type
    for func in functions:
        funcname = func["funcname"].lower()
        for key in bottlenecks:
            if key in funcname:
                bottlenecks[key].append(func)

    # Print statistics for each bottleneck category
    for key, funcs in bottlenecks.items():
        if funcs:
            total_calls = sum(f["ncalls"] for f in funcs)
            total_time = sum(f["cumtime"] for f in funcs)
            print(f"\n{key.upper()}:")
            print(f"  Total calls: {total_calls:,}")
            print(f"  Total time: {total_time:.3f}s ({total_time/log_data['total_time']*100:.1f}%)")
            if funcs:
                print(f"  Main sources:")
                for f in sorted(funcs, key=lambda x: x["cumtime"], reverse=True)[:3]:
                    print(f"    - {f['fullname']}: {f['ncalls']:,} calls, {f['cumtime']:.3f}s")

    return bottlenecks


def compare_with_baseline(current_log, baseline_file):
    """Compare current performance with baseline measurements.

    Generates a detailed comparison report showing:
    - Total execution time difference (absolute and percentage)
    - Total function call count difference
    - Per-function call count changes for key operations

    Args:
        current_log (dict): Parsed log data from current run.
        baseline_file (str): Path to baseline log file for comparison.

    Side Effects:
        Prints comparison report to stdout.

    Example:
        >>> current_data = parse_log_file("current_profile.log")
        >>> compare_with_baseline(current_data, "baseline_profile.log")
        This will print a comparison table showing performance changes.
    """
    try:
        baseline_data = parse_log_file(baseline_file)

        print("\n" + "=" * 100)
        print(f"Comparison with baseline: {baseline_file}")
        print("=" * 100)

        print(f"\n{'Metric':<30} {'Baseline':<20} {'Current':<20} {'Change':<20}")
        print("-" * 100)

        # Total execution time comparison
        time_diff = current_log["total_time"] - baseline_data["total_time"]
        time_pct = (
            (time_diff / baseline_data["total_time"] * 100)
            if baseline_data["total_time"] > 0
            else 0
        )
        print(
            f"{'Total execution time':<30} {baseline_data['total_time']:>19.2f}s {current_log['total_time']:>19.2f}s {time_diff:+19.2f}s ({time_pct:+.1f}%)"
        )

        # Total calls comparison
        calls_diff = current_log["total_calls"] - baseline_data["total_calls"]
        calls_pct = (
            (calls_diff / baseline_data["total_calls"] * 100)
            if baseline_data["total_calls"] > 0
            else 0
        )
        print(
            f"{'Total function calls':<30} {baseline_data['total_calls']:>19,} {current_log['total_calls']:>19,} {calls_diff:+19,} ({calls_pct:+.1f}%)"
        )

        # Key function comparison
        print("\nKey Function Call Count Comparison:")
        print(f"{'Function':<30} {'Baseline Calls':<20} {'Current Calls':<20} {'Change':<20}")
        print("-" * 100)

        key_functions = [
            "hasattr",
            "getattr",
            "setattr",
            "isinstance",
            "__getattr__",
            "__setattr__",
            "__getitem__",
        ]

        for key in key_functions:
            # Find all functions matching the key in both baseline and current
            baseline_funcs = [f for f in baseline_data["functions"] if key in f["funcname"].lower()]
            current_funcs = [f for f in current_log["functions"] if key in f["funcname"].lower()]

            baseline_calls = sum(f["ncalls"] for f in baseline_funcs)
            current_calls = sum(f["ncalls"] for f in current_funcs)

            if baseline_calls > 0 or current_calls > 0:
                diff = current_calls - baseline_calls
                pct = (diff / baseline_calls * 100) if baseline_calls > 0 else float("inf")
                if pct == float("inf"):
                    print(f"{key:<30} {baseline_calls:>19,} {current_calls:>19,} {diff:+19,} (NEW)")
                else:
                    print(
                        f"{key:<30} {baseline_calls:>19,} {current_calls:>19,} {diff:+19,} ({pct:+.1f}%)"
                    )

    except FileNotFoundError:
        print(f"\nWarning: Baseline file not found {baseline_file}")
    except Exception as e:
        print(f"\nError: Comparison failed - {e}")


def generate_optimization_recommendations(bottlenecks, log_data):
    """Generate prioritized optimization recommendations based on bottleneck analysis.

    Analyzes the identified bottlenecks and generates actionable optimization
    recommendations with:
    - Priority level (1=High, 2=Medium, 3=Low)
    - Problem description
    - Solution approach
    - Expected performance gain
    - Affected files

    Args:
        bottlenecks (dict): Bottleneck analysis from analyze_bottlenecks().
        log_data (dict): Parsed log data from parse_log_file().

    Returns:
        list: A list of recommendation dictionaries, each containing:
            - priority (int): Priority level (1-3)
            - title (str): Brief recommendation title
            - issue (str): Description of the performance issue
            - solution (str): Suggested optimization approach
            - expected_gain (str): Expected performance improvement
            - files (list): List of files to modify

    Side Effects:
        Prints detailed recommendations report to stdout.

    Example:
        >>> bottlenecks = analyze_bottlenecks(log_data)
        >>> recommendations = generate_optimization_recommendations(bottlenecks, log_data)
        >>> print(f"Generated {len(recommendations)} recommendations")
    """
    print("\n" + "=" * 100)
    print("Optimization Recommendations (by priority)")
    print("=" * 100)

    recommendations = []

    # Analyze hasattr - Look Before You Leap anti-pattern
    # Recommendation: Use try-except (Easier to Ask Forgiveness than Permission)
    if bottlenecks["hasattr"]:
        total_calls = sum(f["ncalls"] for f in bottlenecks["hasattr"])
        total_time = sum(f["cumtime"] for f in bottlenecks["hasattr"])
        if total_calls > 5000000:  # Threshold: 5 million calls
            recommendations.append(
                {
                    "priority": 1,
                    "title": "Optimize hasattr calls",
                    "issue": f"hasattr called {total_calls:,} times, taking {total_time:.2f}s",
                    "solution": "Use try-except (EAFP) instead of hasattr (LBYL)",
                    "expected_gain": f"Reduce {total_calls*0.7:,.0f} calls, save {total_time*0.7:.1f}s",
                    "files": [
                        "backtrader/lineseries.py",
                        "backtrader/linebuffer.py",
                        "backtrader/lineiterator.py",
                    ],
                }
            )

    # Analyze __getattr__ - Custom attribute access overhead
    # Recommendation: Cache frequently accessed attributes
    if bottlenecks["__getattr__"]:
        total_calls = sum(f["ncalls"] for f in bottlenecks["__getattr__"])
        total_time = sum(f["cumtime"] for f in bottlenecks["__getattr__"])
        if total_calls > 500000:  # Threshold: 500k calls
            recommendations.append(
                {
                    "priority": 1,
                    "title": "Implement __getattr__ attribute caching",
                    "issue": f"__getattr__ called {total_calls:,} times, taking {total_time:.2f}s",
                    "solution": "Cache attributes to __dict__ after first access to avoid repeated lookups",
                    "expected_gain": f"Reduce {total_calls*0.8:,.0f} calls, save {total_time*0.6:.1f}s",
                    "files": ["backtrader/lineseries.py"],
                }
            )

    # Analyze __setattr__ - Custom attribute setting overhead
    # Recommendation: Optimize fast path for simple types
    if bottlenecks["__setattr__"]:
        total_calls = sum(f["ncalls"] for f in bottlenecks["__setattr__"])
        total_time = sum(f["cumtime"] for f in bottlenecks["__setattr__"])
        if total_calls > 1000000:  # Threshold: 1 million calls
            recommendations.append(
                {
                    "priority": 2,
                    "title": "Optimize __setattr__ performance",
                    "issue": f"__setattr__ called {total_calls:,} times, taking {total_time:.2f}s",
                    "solution": "Use fast path for simple types, reduce internal hasattr calls",
                    "expected_gain": f"Save {total_time*0.5:.1f}s",
                    "files": ["backtrader/lineseries.py"],
                }
            )

    # Analyze isinstance/isnan - Type checking overhead
    # Recommendation: Use NaN self-comparison property (value != value)
    isinstance_calls = sum(f["ncalls"] for f in bottlenecks["isinstance"])
    isnan_calls = sum(f["ncalls"] for f in bottlenecks["isnan"])
    if isinstance_calls > 5000000 or isnan_calls > 2000000:
        isinstance_time = sum(f["cumtime"] for f in bottlenecks["isinstance"])
        isnan_time = sum(f["cumtime"] for f in bottlenecks["isnan"])
        recommendations.append(
            {
                "priority": 2,
                "title": "Optimize isinstance/isnan checks",
                "issue": f"isinstance: {isinstance_calls:,} calls, isnan: {isnan_calls:,} calls",
                "solution": "Use value != value to detect NaN (NaN self-comparison property)",
                "expected_gain": f"Reduce {(isinstance_calls+isnan_calls):,.0f} calls, save {isinstance_time+isnan_time:.1f}s",
                "files": ["backtrader/lineseries.py", "backtrader/linebuffer.py"],
            }
        )

    # Analyze __getitem__ - Array indexing overhead
    # Recommendation: Simplify logic, reduce type checks, use direct array access
    if bottlenecks["__getitem__"]:
        total_calls = sum(f["ncalls"] for f in bottlenecks["__getitem__"])
        total_time = sum(f["cumtime"] for f in bottlenecks["__getitem__"])
        if total_time > 3.0:  # Threshold: 3 seconds
            recommendations.append(
                {
                    "priority": 2,
                    "title": "Optimize __getitem__ method",
                    "issue": f"__getitem__ called {total_calls:,} times, taking {total_time:.2f}s",
                    "solution": "Simplify logic, reduce type checks, use direct array access",
                    "expected_gain": f"Save {total_time*0.5:.1f}s",
                    "files": ["backtrader/lineseries.py", "backtrader/linebuffer.py"],
                }
            )

    # Analyze forward - Data propagation overhead
    # Recommendation: Reduce NaN checks, optimize array operations
    if bottlenecks["forward"]:
        total_calls = sum(f["ncalls"] for f in bottlenecks["forward"])
        total_time = sum(f["cumtime"] for f in bottlenecks["forward"])
        if total_time > 5.0:  # Threshold: 5 seconds
            recommendations.append(
                {
                    "priority": 3,
                    "title": "Optimize forward method",
                    "issue": f"forward called {total_calls:,} times, taking {total_time:.2f}s",
                    "solution": "Reduce NaN checks, optimize array operations",
                    "expected_gain": f"Save {total_time*0.3:.1f}s",
                    "files": ["backtrader/linebuffer.py", "backtrader/lineseries.py"],
                }
            )

    # Sort by priority (1=High, 2=Medium, 3=Low)
    recommendations.sort(key=lambda x: x["priority"])

    # Print formatted recommendations
    for i, rec in enumerate(recommendations, 1):
        # Visual priority indicators
        priority_symbol = {
            1: "\U0001f534",  # Red circle
            2: "\U0001f7e1",  # Yellow circle
            3: "\U0001f7e2",  # Green circle
        }
        symbol = priority_symbol.get(rec['priority'], '\u25CF')  # Default black circle

        print(
            f"\n{symbol} Recommendation #{i}: {rec['title']}"
        )
        print(
            f"   Priority: {'High' if rec['priority'] == 1 else 'Medium' if rec['priority'] == 2 else 'Low'}"
        )
        print(f"   Issue: {rec['issue']}")
        print(f"   Solution: {rec['solution']}")
        print(f"   Expected Gain: {rec['expected_gain']}")
        print(f"   Files: {', '.join(rec['files'])}")

    # Calculate total expected improvement
    print("\n" + "=" * 100)
    print("Total Expected Optimization Results")
    print("=" * 100)

    total_expected_time_save = 0
    for rec in recommendations:
        # Extract seconds from expected_gain string
        match = re.search(r"Save ([\d.]+)s", rec["expected_gain"])
        if match:
            total_expected_time_save += float(match.group(1))

    current_time = log_data["total_time"]
    expected_time = current_time - total_expected_time_save
    improvement_pct = (total_expected_time_save / current_time * 100) if current_time > 0 else 0

    print(f"\nCurrent execution time: {current_time:.2f}s")
    print(f"Expected time savings: {total_expected_time_save:.2f}s")
    print(f"Optimized time: {expected_time:.2f}s")
    print(f"Performance improvement: {improvement_pct:.1f}%")

    return recommendations


def main():
    """Main execution function for performance analysis tool.

    This function:
    1. Locates the latest performance log file
    2. Parses and analyzes the log data
    3. Compares with baseline measurements
    4. Generates optimization recommendations
    5. Saves analysis report to markdown file

    Returns:
        int: Exit code (0 for success, 1 for error).

    Example:
        >>> python tools/analyze_current_performance.py
        This will analyze performance logs and generate a report.
    """
    import glob

    # Find latest performance log file
    log_files = glob.glob("performance_profile_remove-metaprogramming_*.log")
    if not log_files:
        print("Error: Performance log file not found")
        return 1

    # Use most recent log file
    current_log_file = sorted(log_files)[-1]
    print(f"Analyzing log file: {current_log_file}")

    # Parse and analyze
    current_data = parse_log_file(current_log_file)
    bottlenecks = analyze_bottlenecks(current_data)

    # Compare with master baseline
    master_log = "performance_profile_master_20251026_230910.log"
    compare_with_baseline(current_data, master_log)

    # Generate optimization recommendations
    recommendations = generate_optimization_recommendations(bottlenecks, current_data)

    # Save report to markdown
    report_file = "current_performance_analysis_report.md"
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(f"# Current Performance Analysis Report\n\n")
        f.write(f"## Basic Information\n\n")
        f.write(f"- Log file: {current_log_file}\n")
        f.write(f"- Total execution time: {current_data['total_time']:.2f}s\n")
        f.write(f"- Total function calls: {current_data['total_calls']:,}\n")
        f.write(
            f"- Analysis time: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        )

        f.write(f"## Optimization Recommendations\n\n")
        for i, rec in enumerate(recommendations, 1):
            f.write(f"### {i}. {rec['title']}\n\n")
            priority_emoji = {
                1: 'High \U0001f534',
                2: 'Medium \U0001f7e1',
                3: 'Low \U0001f7e2',
            }
            emoji = priority_emoji.get(rec['priority'], 'Unknown')
            f.write(
                f"**Priority**: {emoji}\n\n"
            )
            f.write(f"**Issue**: {rec['issue']}\n\n")
            f.write(f"**Solution**: {rec['solution']}\n\n")
            f.write(f"**Expected Gain**: {rec['expected_gain']}\n\n")
            f.write(f"**Files**: {', '.join(rec['files'])}\n\n")

        f.write(f"\n## Detailed Data\n\n")
        f.write(f"See full performance log: {current_log_file}\n")

    print(f"\nReport saved to: {report_file}")

    return 0


if __name__ == "__main__":
    sys.exit(main())

#!/usr/bin/env python3
"""
Backtrader Class Dependency Analyzer

åˆ†æbacktraderé¡¹ç›®ä¸­çš„ç±»ä¾èµ–å…³ç³»ï¼Œç”Ÿæˆä¾èµ–å›¾ï¼Œè¯†åˆ«å…³é”®è·¯å¾„å’Œé£é™©ç‚¹ã€‚
"""

import os
import sys
import ast
import json
import re
from typing import Dict, List, Set, Any, Tuple, Optional
from datetime import datetime
from dataclasses import dataclass, field
from collections import defaultdict, deque
import networkx as nx

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


@dataclass
class ClassInfo:
    """ç±»ä¿¡æ¯"""
    name: str
    file_path: str
    line_number: int
    base_classes: List[str] = field(default_factory=list)
    metaclass: Optional[str] = None
    imports: List[str] = field(default_factory=list)
    methods: List[str] = field(default_factory=list)
    attributes: List[str] = field(default_factory=list)
    has_metaprogramming: bool = False
    metaprogramming_types: List[str] = field(default_factory=list)


@dataclass
class DependencyEdge:
    """ä¾èµ–è¾¹"""
    source: str
    target: str
    edge_type: str  # 'inheritance', 'composition', 'import', 'metaclass'
    file_path: str
    line_number: int


@dataclass
class RiskAssessment:
    """é£é™©è¯„ä¼°"""
    class_name: str
    risk_level: str  # 'low', 'medium', 'high', 'critical'
    risk_factors: List[str]
    dependent_classes: int
    complexity_score: float


class ClassDependencyAnalyzer(ast.NodeVisitor):
    """ç±»ä¾èµ–åˆ†æå™¨"""
    
    def __init__(self, file_path: str):
        self.file_path = file_path
        self.classes: Dict[str, ClassInfo] = {}
        self.dependencies: List[DependencyEdge] = []
        self.imports: Dict[str, str] = {}  # alias -> full_name
        self.current_class: Optional[str] = None
        
        # å·²çŸ¥çš„å…ƒç±»
        self.known_metaclasses = {
            'MetaBase', 'MetaParams', 'MetaLineSeries', 'MetaSingleton',
            'MetaIndicator', 'MetaStrategy', 'MetaAnalyzer', 'MetaObserver'
        }
    
    def visit_Import(self, node: ast.Import):
        """å¤„ç†importè¯­å¥"""
        for alias in node.names:
            name = alias.name
            as_name = alias.asname or name.split('.')[-1]
            self.imports[as_name] = name
        self.generic_visit(node)
    
    def visit_ImportFrom(self, node: ast.ImportFrom):
        """å¤„ç†from importè¯­å¥"""
        module = node.module or ''
        for alias in node.names:
            name = alias.name
            as_name = alias.asname or name
            full_name = f"{module}.{name}" if module else name
            self.imports[as_name] = full_name
        self.generic_visit(node)
    
    def visit_ClassDef(self, node: ast.ClassDef):
        """è®¿é—®ç±»å®šä¹‰"""
        class_name = node.name
        self.current_class = class_name
        
        # è§£æåŸºç±»
        base_classes = []
        for base in node.bases:
            base_name = self._get_name(base)
            if base_name:
                # è§£æå¯¼å…¥çš„åŸºç±»
                resolved_name = self.imports.get(base_name, base_name)
                base_classes.append(resolved_name)
                
                # æ·»åŠ ç»§æ‰¿ä¾èµ–
                self.dependencies.append(DependencyEdge(
                    source=class_name,
                    target=base_name,
                    edge_type='inheritance',
                    file_path=self.file_path,
                    line_number=node.lineno
                ))
        
        # è§£æå…ƒç±»
        metaclass = None
        if hasattr(node, 'metaclass') and node.metaclass:
            metaclass = self._get_name(node.metaclass)
            if metaclass:
                self.dependencies.append(DependencyEdge(
                    source=class_name,
                    target=metaclass,
                    edge_type='metaclass',
                    file_path=self.file_path,
                    line_number=node.lineno
                ))
        
        # æ£€æŸ¥å…ƒç¼–ç¨‹ä½¿ç”¨
        has_metaprogramming = False
        metaprogramming_types = []
        
        if metaclass and metaclass in self.known_metaclasses:
            has_metaprogramming = True
            metaprogramming_types.append(f'metaclass:{metaclass}')
        
        for base in base_classes:
            if any(meta in base for meta in self.known_metaclasses):
                has_metaprogramming = True
                metaprogramming_types.append(f'inheritance:{base}')
        
        # åˆ›å»ºç±»ä¿¡æ¯
        class_info = ClassInfo(
            name=class_name,
            file_path=self.file_path,
            line_number=node.lineno,
            base_classes=base_classes,
            metaclass=metaclass,
            imports=list(self.imports.keys()),
            has_metaprogramming=has_metaprogramming,
            metaprogramming_types=metaprogramming_types
        )
        
        # åˆ†æç±»ä½“
        self._analyze_class_body(node, class_info)
        
        self.classes[class_name] = class_info
        self.generic_visit(node)
        self.current_class = None
    
    def _analyze_class_body(self, node: ast.ClassDef, class_info: ClassInfo):
        """åˆ†æç±»ä½“"""
        for item in node.body:
            if isinstance(item, ast.FunctionDef):
                class_info.methods.append(item.name)
            elif isinstance(item, ast.Assign):
                for target in item.targets:
                    if isinstance(target, ast.Name):
                        class_info.attributes.append(target.id)
                        
                        # æ£€æŸ¥ç‰¹æ®Šå±æ€§
                        if target.id in ['params', 'lines']:
                            class_info.has_metaprogramming = True
                            class_info.metaprogramming_types.append(f'attribute:{target.id}')
    
    def visit_Call(self, node: ast.Call):
        """è®¿é—®å‡½æ•°è°ƒç”¨"""
        if self.current_class:
            func_name = self._get_name(node.func)
            if func_name and '.' in func_name:
                # å¯èƒ½æ˜¯å¯¹å…¶ä»–ç±»çš„è°ƒç”¨
                parts = func_name.split('.')
                if len(parts) >= 2:
                    target_class = parts[0]
                    if target_class in self.imports:
                        resolved_target = self.imports[target_class]
                        self.dependencies.append(DependencyEdge(
                            source=self.current_class,
                            target=resolved_target,
                            edge_type='composition',
                            file_path=self.file_path,
                            line_number=node.lineno
                        ))
        
        self.generic_visit(node)
    
    def _get_name(self, node):
        """è·å–èŠ‚ç‚¹åç§°"""
        if isinstance(node, ast.Name):
            return node.id
        elif isinstance(node, ast.Attribute):
            value = self._get_name(node.value)
            return f"{value}.{node.attr}" if value else node.attr
        elif isinstance(node, ast.Constant):
            return str(node.value)
        return None


class ProjectDependencyAnalyzer:
    """é¡¹ç›®ä¾èµ–åˆ†æå™¨"""
    
    def __init__(self, project_root: str = '.'):
        self.project_root = os.path.abspath(project_root)
        self.all_classes: Dict[str, ClassInfo] = {}
        self.all_dependencies: List[DependencyEdge] = []
        self.dependency_graph = nx.DiGraph()
        self.inheritance_graph = nx.DiGraph()
        self.composition_graph = nx.DiGraph()
        self.risk_assessments: List[RiskAssessment] = []
    
    def analyze_project(self):
        """åˆ†ææ•´ä¸ªé¡¹ç›®"""
        print("å¼€å§‹åˆ†æbacktraderé¡¹ç›®çš„ç±»ä¾èµ–å…³ç³»...")
        
        # éå†é¡¹ç›®ä¸­çš„Pythonæ–‡ä»¶
        for root, dirs, files in os.walk(self.project_root):
            # è·³è¿‡ä¸€äº›ç›®å½•
            dirs[:] = [d for d in dirs if not d.startswith('.') and d not in ['__pycache__', 'build', 'dist']]
            
            for file in files:
                if file.endswith('.py'):
                    file_path = os.path.join(root, file)
                    rel_path = os.path.relpath(file_path, self.project_root)
                    
                    # åªåˆ†æbacktraderåŒ…å†…çš„æ–‡ä»¶
                    if rel_path.startswith('backtrader' + os.sep) or file == '__init__.py':
                        try:
                            self.analyze_file(file_path, rel_path)
                        except Exception as e:
                            print(f"åˆ†ææ–‡ä»¶ {rel_path} æ—¶å‡ºé”™: {e}")
        
        # æ„å»ºå›¾
        self.build_graphs()
        
        # é£é™©è¯„ä¼°
        self.assess_risks()
    
    def analyze_file(self, file_path: str, rel_path: str):
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            tree = ast.parse(content)
            analyzer = ClassDependencyAnalyzer(rel_path)
            analyzer.visit(tree)
            
            # æ”¶é›†ç»“æœ
            self.all_classes.update(analyzer.classes)
            self.all_dependencies.extend(analyzer.dependencies)
            
        except Exception as e:
            print(f"æ— æ³•è§£ææ–‡ä»¶ {rel_path}: {e}")
    
    def build_graphs(self):
        """æ„å»ºä¾èµ–å›¾"""
        # æ€»ä¾èµ–å›¾
        for dep in self.all_dependencies:
            self.dependency_graph.add_edge(dep.source, dep.target, 
                                         edge_type=dep.edge_type,
                                         file_path=dep.file_path,
                                         line_number=dep.line_number)
        
        # ç»§æ‰¿å›¾
        for dep in self.all_dependencies:
            if dep.edge_type in ['inheritance', 'metaclass']:
                self.inheritance_graph.add_edge(dep.source, dep.target,
                                              edge_type=dep.edge_type)
        
        # ç»„åˆå›¾
        for dep in self.all_dependencies:
            if dep.edge_type == 'composition':
                self.composition_graph.add_edge(dep.source, dep.target)
    
    def assess_risks(self):
        """è¯„ä¼°é‡æ„é£é™©"""
        self.risk_assessments = []
        
        for class_name, class_info in self.all_classes.items():
            risk_factors = []
            risk_level = 'low'
            complexity_score = 0.0
            
            # è®¡ç®—ä¾èµ–æ­¤ç±»çš„ç±»çš„æ•°é‡
            dependent_classes = len([dep for dep in self.all_dependencies 
                                   if dep.target == class_name])
            
            # é£é™©å› ç´ è¯„ä¼°
            if class_info.has_metaprogramming:
                risk_factors.append('ä½¿ç”¨å…ƒç¼–ç¨‹')
                complexity_score += 2.0
            
            if class_info.metaclass:
                risk_factors.append(f'ä½¿ç”¨å…ƒç±»: {class_info.metaclass}')
                complexity_score += 3.0
            
            if len(class_info.base_classes) > 2:
                risk_factors.append('å¤šé‡ç»§æ‰¿')
                complexity_score += 1.5
            
            if dependent_classes > 10:
                risk_factors.append(f'è¢«{dependent_classes}ä¸ªç±»ä¾èµ–')
                complexity_score += 1.0
            
            if len(class_info.methods) > 20:
                risk_factors.append('æ–¹æ³•æ•°é‡è¿‡å¤š')
                complexity_score += 0.5
            
            # æ£€æŸ¥æ˜¯å¦åœ¨å…³é”®è·¯å¾„ä¸Š
            if self.is_in_critical_path(class_name):
                risk_factors.append('ä½äºå…³é”®è·¯å¾„')
                complexity_score += 2.0
            
            # ç¡®å®šé£é™©çº§åˆ«
            if complexity_score >= 6.0:
                risk_level = 'critical'
            elif complexity_score >= 4.0:
                risk_level = 'high'
            elif complexity_score >= 2.0:
                risk_level = 'medium'
            
            assessment = RiskAssessment(
                class_name=class_name,
                risk_level=risk_level,
                risk_factors=risk_factors,
                dependent_classes=dependent_classes,
                complexity_score=complexity_score
            )
            
            self.risk_assessments.append(assessment)
        
        # æŒ‰é£é™©çº§åˆ«æ’åº
        self.risk_assessments.sort(key=lambda x: x.complexity_score, reverse=True)
    
    def is_in_critical_path(self, class_name: str) -> bool:
        """æ£€æŸ¥ç±»æ˜¯å¦åœ¨å…³é”®è·¯å¾„ä¸Š"""
        # å…³é”®ç±»åˆ—è¡¨ï¼ˆæ ¸å¿ƒæ¡†æ¶ç±»ï¼‰
        critical_classes = {
            'Cerebro', 'Strategy', 'Indicator', 'LineSeriesBase', 'DataBase',
            'BrokerBase', 'FeedBase', 'MetaParams', 'MetaLineSeries'
        }
        
        if class_name in critical_classes:
            return True
        
        # æ£€æŸ¥æ˜¯å¦ç»§æ‰¿è‡ªå…³é”®ç±»
        try:
            for critical in critical_classes:
                if nx.has_path(self.inheritance_graph, class_name, critical):
                    return True
                if nx.has_path(self.inheritance_graph, critical, class_name):
                    return True
        except:
            pass
        
        return False
    
    def find_inheritance_chains(self) -> Dict[str, List[str]]:
        """æŸ¥æ‰¾ç»§æ‰¿é“¾"""
        chains = {}
        
        for class_name in self.all_classes:
            if class_name not in chains:
                chain = self._get_inheritance_chain(class_name)
                if len(chain) > 1:
                    chains[class_name] = chain
        
        return chains
    
    def _get_inheritance_chain(self, class_name: str) -> List[str]:
        """è·å–å•ä¸ªç±»çš„ç»§æ‰¿é“¾"""
        chain = [class_name]
        current = class_name
        
        while True:
            parents = [dep.target for dep in self.all_dependencies 
                      if dep.source == current and dep.edge_type == 'inheritance']
            if not parents:
                break
            
            # é€‰æ‹©ç¬¬ä¸€ä¸ªçˆ¶ç±»ï¼ˆç®€åŒ–å¤„ç†ï¼‰
            parent = parents[0]
            if parent in chain:  # é¿å…å¾ªç¯
                break
            
            chain.append(parent)
            current = parent
        
        return chain
    
    def identify_critical_paths(self) -> List[List[str]]:
        """è¯†åˆ«å…³é”®è·¯å¾„"""
        critical_paths = []
        
        # æ‰¾åˆ°æ ¸å¿ƒèŠ‚ç‚¹
        core_nodes = ['Strategy', 'Indicator', 'Cerebro', 'DataBase', 'BrokerBase']
        
        for core in core_nodes:
            if core in self.dependency_graph:
                # æ‰¾åˆ°æ‰€æœ‰åˆ°è¾¾è¿™ä¸ªæ ¸å¿ƒèŠ‚ç‚¹çš„è·¯å¾„
                for node in self.dependency_graph.nodes():
                    if node != core:
                        try:
                            if nx.has_path(self.dependency_graph, node, core):
                                path = nx.shortest_path(self.dependency_graph, node, core)
                                if len(path) > 2:  # åªå…³æ³¨è¾ƒé•¿çš„è·¯å¾„
                                    critical_paths.append(path)
                        except:
                            continue
        
        return critical_paths
    
    def generate_priority_matrix(self) -> Dict[str, Dict[str, Any]]:
        """ç”Ÿæˆå®æ–½ä¼˜å…ˆçº§çŸ©é˜µ"""
        priority_matrix = {}
        
        # æŒ‰é£é™©çº§åˆ«åˆ†ç»„
        for assessment in self.risk_assessments:
            class_info = self.all_classes.get(assessment.class_name)
            if not class_info:
                continue
            
            priority_info = {
                'risk_level': assessment.risk_level,
                'complexity_score': assessment.complexity_score,
                'dependent_classes': assessment.dependent_classes,
                'metaprogramming_types': class_info.metaprogramming_types,
                'base_classes': class_info.base_classes,
                'file_path': class_info.file_path,
                'suggested_phase': self._suggest_refactor_phase(assessment, class_info),
                'prerequisites': self._find_prerequisites(class_info),
                'impact_scope': self._assess_impact_scope(assessment.class_name)
            }
            
            priority_matrix[assessment.class_name] = priority_info
        
        return priority_matrix
    
    def _suggest_refactor_phase(self, assessment: RiskAssessment, class_info: ClassInfo) -> str:
        """å»ºè®®é‡æ„é˜¶æ®µ"""
        # æ ¹æ®å…ƒç¼–ç¨‹ç±»å‹å»ºè®®é˜¶æ®µ
        for mp_type in class_info.metaprogramming_types:
            if 'MetaSingleton' in mp_type:
                return 'Phase 2: Singletoné‡æ„'
            elif 'MetaParams' in mp_type or 'params' in mp_type:
                return 'Phase 3: å‚æ•°ç³»ç»Ÿé‡æ„'
            elif 'MetaLineSeries' in mp_type or 'lines' in mp_type:
                return 'Phase 4: Linesç³»ç»Ÿé‡æ„'
        
        # æ ¹æ®é£é™©çº§åˆ«
        if assessment.risk_level == 'critical':
            return 'Phase 1: ç´§æ€¥å¤„ç†'
        elif assessment.risk_level == 'high':
            return 'Phase 2-3: é«˜ä¼˜å…ˆçº§'
        else:
            return 'Phase 4-5: åæœŸå¤„ç†'
    
    def _find_prerequisites(self, class_info: ClassInfo) -> List[str]:
        """æŸ¥æ‰¾é‡æ„å‰ç½®æ¡ä»¶"""
        prerequisites = []
        
        # åŸºç±»å¿…é¡»å…ˆé‡æ„
        for base in class_info.base_classes:
            if any(meta in base for meta in ['Meta', 'Base']):
                prerequisites.append(f'é‡æ„åŸºç±»: {base}')
        
        # å…ƒç±»å¿…é¡»å…ˆé‡æ„
        if class_info.metaclass:
            prerequisites.append(f'é‡æ„å…ƒç±»: {class_info.metaclass}')
        
        return prerequisites
    
    def _assess_impact_scope(self, class_name: str) -> str:
        """è¯„ä¼°å½±å“èŒƒå›´"""
        dependent_count = len([dep for dep in self.all_dependencies 
                             if dep.target == class_name])
        
        if dependent_count >= 20:
            return 'å…¨é¡¹ç›®å½±å“'
        elif dependent_count >= 10:
            return 'æ¨¡å—çº§å½±å“'
        elif dependent_count >= 5:
            return 'å±€éƒ¨å½±å“'
        else:
            return 'æœ€å°å½±å“'
    
    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        inheritance_chains = self.find_inheritance_chains()
        critical_paths = self.identify_critical_paths()
        priority_matrix = self.generate_priority_matrix()
        
        report = {
            'analysis_time': datetime.now().isoformat(),
            'summary': {
                'total_classes': len(self.all_classes),
                'total_dependencies': len(self.all_dependencies),
                'inheritance_edges': len([d for d in self.all_dependencies if d.edge_type == 'inheritance']),
                'composition_edges': len([d for d in self.all_dependencies if d.edge_type == 'composition']),
                'metaclass_edges': len([d for d in self.all_dependencies if d.edge_type == 'metaclass']),
                'classes_with_metaprogramming': len([c for c in self.all_classes.values() if c.has_metaprogramming]),
                'critical_risk_classes': len([r for r in self.risk_assessments if r.risk_level == 'critical']),
                'high_risk_classes': len([r for r in self.risk_assessments if r.risk_level == 'high'])
            },
            'risk_assessments': [
                {
                    'class_name': r.class_name,
                    'risk_level': r.risk_level,
                    'complexity_score': r.complexity_score,
                    'dependent_classes': r.dependent_classes,
                    'risk_factors': r.risk_factors
                }
                for r in self.risk_assessments[:20]  # Top 20
            ],
            'inheritance_chains': {k: v for k, v in inheritance_chains.items() if len(v) > 3},
            'critical_paths': critical_paths[:10],  # Top 10
            'priority_matrix': priority_matrix,
            'phase_distribution': self._get_phase_distribution(priority_matrix),
            'dependency_statistics': self._get_dependency_statistics()
        }
        
        return report
    
    def _get_phase_distribution(self, priority_matrix: Dict) -> Dict[str, int]:
        """è·å–å„é˜¶æ®µçš„ç±»åˆ†å¸ƒ"""
        distribution = defaultdict(int)
        for info in priority_matrix.values():
            distribution[info['suggested_phase']] += 1
        return dict(distribution)
    
    def _get_dependency_statistics(self) -> Dict[str, Any]:
        """è·å–ä¾èµ–ç»Ÿè®¡ä¿¡æ¯"""
        # å…¥åº¦ç»Ÿè®¡ï¼ˆè¢«ä¾èµ–ï¼‰
        in_degrees = defaultdict(int)
        out_degrees = defaultdict(int)
        
        for dep in self.all_dependencies:
            in_degrees[dep.target] += 1
            out_degrees[dep.source] += 1
        
        # æœ€é«˜å…¥åº¦ï¼ˆæœ€è¢«ä¾èµ–çš„ç±»ï¼‰
        top_depended = sorted(in_degrees.items(), key=lambda x: x[1], reverse=True)[:10]
        
        # æœ€é«˜å‡ºåº¦ï¼ˆä¾èµ–æœ€å¤šå…¶ä»–ç±»çš„ç±»ï¼‰
        top_depending = sorted(out_degrees.items(), key=lambda x: x[1], reverse=True)[:10]
        
        return {
            'most_depended_classes': top_depended,
            'most_depending_classes': top_depending,
            'average_in_degree': sum(in_degrees.values()) / len(in_degrees) if in_degrees else 0,
            'average_out_degree': sum(out_degrees.values()) / len(out_degrees) if out_degrees else 0
        }
    
    def save_report(self, report: Dict[str, Any]):
        """ä¿å­˜åˆ†ææŠ¥å‘Š"""
        os.makedirs('analysis_results', exist_ok=True)
        filename = f"analysis_results/dependency_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ä¾èµ–åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
        return filename
    
    def print_summary(self, report: Dict[str, Any]):
        """æ‰“å°åˆ†ææ‘˜è¦"""
        print("="*70)
        print("Backtrader ç±»ä¾èµ–å…³ç³»åˆ†ææŠ¥å‘Š")
        print("="*70)
        
        summary = report['summary']
        print(f"æ€»ç±»æ•°: {summary['total_classes']}")
        print(f"æ€»ä¾èµ–å…³ç³»: {summary['total_dependencies']}")
        print(f"  - ç»§æ‰¿å…³ç³»: {summary['inheritance_edges']}")
        print(f"  - ç»„åˆå…³ç³»: {summary['composition_edges']}")
        print(f"  - å…ƒç±»å…³ç³»: {summary['metaclass_edges']}")
        print(f"ä½¿ç”¨å…ƒç¼–ç¨‹çš„ç±»: {summary['classes_with_metaprogramming']}")
        print(f"å…³é”®é£é™©ç±»: {summary['critical_risk_classes']}")
        print(f"é«˜é£é™©ç±»: {summary['high_risk_classes']}")
        
        print("\nğŸš¨ æœ€é«˜é£é™©ç±» (Top 10):")
        print("-"*50)
        print(f"{'ç±»å':<25} {'é£é™©çº§åˆ«':<10} {'å¤æ‚åº¦':<8} {'è¢«ä¾èµ–æ•°':<8}")
        print("-"*50)
        for risk in report['risk_assessments'][:10]:
            print(f"{risk['class_name']:<25} {risk['risk_level']:<10} "
                  f"{risk['complexity_score']:<8.1f} {risk['dependent_classes']:<8}")
        
        print("\nğŸ“Š é˜¶æ®µåˆ†å¸ƒ:")
        print("-"*30)
        for phase, count in report['phase_distribution'].items():
            print(f"{phase}: {count} ä¸ªç±»")
        
        print("\nğŸ”— æœ€è¢«ä¾èµ–çš„ç±» (Top 5):")
        print("-"*30)
        for class_name, count in report['dependency_statistics']['most_depended_classes'][:5]:
            print(f"{class_name}: {count} ä¸ªä¾èµ–")


def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹åˆ†æbacktraderé¡¹ç›®ä¾èµ–å…³ç³»...")
    
    analyzer = ProjectDependencyAnalyzer()
    analyzer.analyze_project()
    
    report = analyzer.generate_report()
    analyzer.print_summary(report)
    analyzer.save_report(report)
    
    print("\nDay 5-7ä»»åŠ¡å®Œæˆï¼")


if __name__ == '__main__':
    # æ£€æŸ¥æ˜¯å¦å®‰è£…äº†networkx
    try:
        import networkx
    except ImportError:
        print("è¯·å®‰è£…networkx: pip install networkx")
        sys.exit(1)
    
    main() 